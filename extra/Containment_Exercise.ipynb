{"cells":[{"cell_type":"markdown","source":"## Calculating Containment\n\nIn this notebook, you'll implement a containment function that looks at a source and answer text and returns a *normalized* value that represents the similarity between those two texts based on their n-gram intersection.","metadata":{"cell_id":"00000-e13294f9-d903-4032-b9ee-fae8436efe61"}},{"cell_type":"code","metadata":{"cell_id":"00001-0ac9cb50-13ea-4870-b213-4ed19d56443c"},"source":"import numpy as np\nimport sklearn","outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### N-gram counts\n\nOne of the first things you'll need to do is to count up the occurrences of n-grams in your text data. To convert a set of text data into a matrix of counts, you can use a [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).\n\nBelow, you can set a value for n and use a CountVectorizer is used to count up the n-gram occurrences. In the next cell, we'll see that the CountVectorizer constructs a vocabulary, and later, we'll look at the matrix of counts.","metadata":{"cell_id":"00002-c1ebf29e-1715-4903-a96e-de4aea02712f"}},{"cell_type":"code","metadata":{"cell_id":"00003-e0c9ef7e-2b2a-4c39-a960-036e5e1b040d"},"source":"from sklearn.feature_extraction.text import CountVectorizer\n\na_text = \"This is an answer text\"\ns_text = \"This is a source text\"\n\n# set n\nn = 1\n\n# instantiate an ngram counter\ncounts = CountVectorizer(analyzer='word', ngram_range=(n,n))\n\n# create a dictionary of n-grams by calling `.fit`\nvocab2int = counts.fit([a_text, s_text]).vocabulary_\n\n# print dictionary of words:index\nprint(vocab2int)","outputs":[{"name":"stdout","text":"{'this': 5, 'is': 2, 'an': 0, 'answer': 1, 'text': 4, 'source': 3}\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"### EXERCISE: Create a vocabulary for 2-grams (aka \"bigrams\")\n\nCreate a `CountVectorizer`, `counts_2grams`, and fit it to our text data. Print out the resultant vocabulary.","metadata":{"cell_id":"00004-56b07569-1d4b-4e8e-9ec3-62a203a2bfb9"}},{"cell_type":"code","metadata":{"cell_id":"00005-72cbab50-6f32-460b-af09-03ba5c3f8778"},"source":"# create a vocabulary for 2-grams\ncounts_2grams = CountVectorizer(analyzer='word', ngram_range=(2,2))\n\n# create a dictionary of n-grams by calling `.fit`\nvocab2int_2grams = counts_2grams.fit([a_text, s_text]).vocabulary_\n\n# print dictionary of words:index\nprint(vocab2int_2grams)","outputs":[{"name":"stdout","text":"{'this is': 5, 'is an': 2, 'an answer': 0, 'answer text': 1, 'is source': 3, 'source text': 4}\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"### What makes up a word?\n\nYou'll note that the word \"a\" does not appear in the vocabulary. And also that the words have been converted to lowercase. When `CountVectorizer` is passed `analyzer='word'` it defines a word as *two or more* characters and so it ignores uni-character words. In a lot of text analysis, single characters are often irrelevant to the meaning of a passage, so leaving them out of a vocabulary is often desired behavior. \n\nFor our purposes, this default behavior will work well; we don't need uni-character words to determine cases of plagiarism, but you may still want to experiment with uni-character counts.\n\n> If you *do* want to include single characters as words, you can choose to do so by adding one more argument when creating the `CountVectorizer`; pass in the definition of a token, `token_pattern = r\"(?u)\\b\\w+\\b\"`. \n\nThis regular expression defines a word as one or more characters. If you want to learn more about this vectorizer, I suggest reading through the [source code](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_extraction/text.py#L664), which is well documented.\n\n**Next, let's fit our `CountVectorizer` to all of our text data to make an array of n-gram counts!**\n\nThe below code, assumes that `counts` is our `CountVectorizer` for the n-gram size we are interested in.","metadata":{"cell_id":"00006-e6868dda-dba6-4559-b966-422f8797e974"}},{"cell_type":"code","metadata":{"cell_id":"00007-be206322-dddd-4bc2-bf0e-415302102db2"},"source":"# create array of n-gram counts for the answer and source text\nngrams = counts.fit_transform([a_text, s_text])\n\n# row = the 2 texts and column = indexed vocab terms (as mapped above)\n# ex. column 0 = 'an', col 1 = 'answer'.. col 4 = 'text'\nngram_array = ngrams.toarray()\nprint(ngram_array)","outputs":[{"name":"stdout","text":"[[1 1 1 0 1 1]\n [0 0 1 1 1 1]]\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"So, the top row indicates the n-gram counts for the answer text `a_text`, and the second row indicates those for the source text `s_text`. If they have n-grams in common, you can see this by looking at the column values. For example they both have one \"is\" (column 2) and \"text\" (column 4) and \"this\" (column 5).\n\n```\n[[1 1 1 0 1 1]    =   an  answer  [is]  ______  [text] [this]\n [0 0 1 1 1 1]]   =   __  ______  [is]  source  [text] [this]\n```","metadata":{"cell_id":"00008-2639aa53-5ceb-4e89-b5a0-2439a7f21df5"}},{"cell_type":"markdown","source":"### EXERCISE: Calculate containment values\n\nAssume your function takes in an `ngram_array` just like that generated above, for an answer text (row 0) and a source text (row 1). Using just this information, calculate the containment between the two texts. As before, it's okay to ignore the uni-character words.\n\nTo calculate the containment:\n1. Calculate the n-gram **intersection** between the answer and source text.\n2. Add up the number of common terms.\n3. Normalize by dividing the value in step 2 by the number of n-grams in the answer text.\n\nThe complete equation is:\n\n$$ \\frac{\\sum{count(\\text{ngram}_{A}) \\cap count(\\text{ngram}_{S})}}{\\sum{count(\\text{ngram}_{A})}} $$","metadata":{"cell_id":"00009-7aab2bbd-deb6-44b5-8ec8-42dd534651bc"}},{"cell_type":"code","metadata":{"cell_id":"00010-e910930b-270a-440c-97b8-99499803a8c5"},"source":"def containment(ngram_array):\n    ''' Containment is a measure of text similarity. It is the normalized, \n       intersection of ngram word counts in two texts.\n       :param ngram_array: an array of ngram counts for an answer and source text.\n       :return: a normalized containment value.'''\n    \n    # your code \n    intersection_ = sum(np.amin(ngram_array,axis=0))\n    union_ = sum(ngram_array[0])\n    \n    return intersection_ / union_\n","outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"cell_id":"00011-d005b789-60f5-4875-99b4-c37ef3dc2433"},"source":"# test out your code\ncontainment_val = containment(ngrams.toarray())\n\nprint('Containment: ', containment_val)\n\n# note that for the given texts, and n = 1\n# the containment value should be 3/5 or 0.6\nassert containment_val==0.6, 'Unexpected containment value for n=1.'\nprint('Test passed!')","outputs":[{"name":"stdout","text":"Containment:  0.6\nTest passed!\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","metadata":{"cell_id":"00012-4075a42c-ed8b-415a-abe4-930be03f7d17"},"source":"# test for n = 2\ncounts_2grams = CountVectorizer(analyzer='word', ngram_range=(2,2))\nbigram_counts = counts_2grams.fit_transform([a_text, s_text])\n\n# calculate containment\ncontainment_val = containment(bigram_counts.toarray())\n\nprint('Containment for n=2 : ', containment_val)\n\n# the containment value should be 1/4 or 0.25\nassert containment_val==0.25, 'Unexpected containment value for n=2.'\nprint('Test passed!')","outputs":[{"name":"stdout","text":"Containment for n=2 :  0.25\nTest passed!\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"I recommend trying out different phrases, and different values of n. What happens if you count for uni-character words? What if you make the sentences much larger?\n\nI find that the best way to understand a new concept is to think about how it might be applied in a variety of different ways.","metadata":{"cell_id":"00013-1e4399d6-84e0-457a-b12a-75d19418013d"}}],"nbformat":4,"nbformat_minor":2,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"},"deepnote_notebook_id":"c081f631-3842-4468-899d-08bb9dcd1c82","deepnote_execution_queue":[]}}